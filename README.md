# Databricks Data Engineering

### Course Overview  
Explore how to leverage the Databricks Lakehouse Platform to productionalize ETL pipelines.  
Use Delta Live Tables with Spark SQL and PySpark to define and schedule pipelines that incrementally process new data from a variety of data sources into the Lakehouse, orchestrate tasks with Databricks Workflows, and promote code with Databricks Repos.  

### Learning Outcomes  
- Use the Databricks Data Science and Engineering Workspace to perform common code development tasks in a data engineering workflow.
- Use Spark SQL/PySpark to extract data from a variety of sources, apply common cleaning transformations, and manipulate complex data with advanced functions.
- Define and schedule data pipelines that incrementally ingest and process data through multiple tables in the lakehouse using Delta Live Tables in Spark SQL/PySpark. 
- Create and manage Databricks jobs with multiple tasks to orchestrate and monitor data workflows.
- Configure permissions in Unity Catalog to ensure that users have proper access to databases for analytics and dashboarding.

### Pre-requisites  
- Beginner familiarity with cloud computing concepts (virtual machines, object storage, etc.)
- Production experience working with data warehouses and data lakes
- Intermediate experience with basic SQL concepts (select, filter, groupby, join, etc)
- Beginner programming experience with Python (syntax, conditions, loops, functions)
- Beginner programming experience with the Spark DataFrame API (PySpark)
- Configure DataFrameReader and DataFrameWriter to read and write data (PySpark)
- Express query transformations using DataFrame methods and Column expressions (PySpark)
- Navigate the Spark documentation to identify built-in functions for various transformations and data types (PySpark)
